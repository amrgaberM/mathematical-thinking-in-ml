# Applied Math in Machine Learning

This module bridges mathematical theory and machine learning practice. It focuses on how core mathematical concepts—especially from calculus, probability, and linear algebra—are directly used in building, training, and optimizing ML models.

## Topics Covered

- **Gradient-Based Optimization**
  - Gradient descent and stochastic gradient descent
  - Loss surfaces and convergence behavior
  - Partial derivatives in weight updates

- **Cost and Loss Functions**
  - Mean squared error (MSE)
  - Cross-entropy loss
  - Log-likelihood functions

- **Regularization Techniques**
  - L1 and L2 regularization
  - Bias-variance tradeoff
  - Penalized loss functions

- **Matrix Operations in ML**
  - Linear regression using matrix algebra
  - Logistic regression with sigmoid and log-odds
  - Multivariate data transformations

- **Backpropagation**
  - Chain rule in action
  - Derivative flow through neural networks
  - Weight and bias updates

- **Model Evaluation Math**
  - Confusion matrix metrics (precision, recall, F1-score)
  - ROC curves and AUC
  - Probability calibration

## Learning Approach

- **Math-First Perspective**  
  Start from equations and derive implementation logic.

- **Python Implementation**  
  All concepts are implemented using NumPy and, where relevant, scikit-learn.

- **Clear Use Cases**  
  Each notebook demonstrates how math drives model behavior, learning, and evaluation.

## Objective

To demonstrate that mathematical understanding is not isolated but critical for effective machine learning. This section highlights the exact points where theory becomes practice.

