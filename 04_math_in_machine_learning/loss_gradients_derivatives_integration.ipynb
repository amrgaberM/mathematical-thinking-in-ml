{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How Derivatives, Gradients, Loss Functions, and Integration Connect in Machine Learning\n",
        "\n",
        "This note explains how basic calculus concepts — **derivatives** and **integration** — are used in machine learning, especially in model training and optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Derivatives and Gradients\n",
        "\n",
        "### What Is a Derivative?\n",
        "\n",
        "A **derivative** tells us how a function changes when its input changes.  \n",
        "In ML, we use derivatives to understand how the **loss function** changes when we update a model's parameters (like weights).\n",
        "\n",
        "Example:\n",
        "If the loss goes up when we increase a weight, the derivative tells us to reduce that weight.\n",
        "\n",
        "---\n",
        "\n",
        "### What Is a Gradient?\n",
        "\n",
        "When a function has many inputs (like many weights), we compute one derivative for each input.  \n",
        "The full set of derivatives is called the **gradient** — it tells us the direction to move in to reduce the loss.\n",
        "\n",
        "---\n",
        "\n",
        "### Gradient Descent (Simplified)\n",
        "\n",
        "To train a model, we use **gradient descent**:\n",
        "\n",
        "\\[\n",
        "\\text{New weight} = \\text{Old weight} - \\text{Learning rate} \\times \\text{Gradient}\n",
        "\\]\n",
        "\n",
        "This repeats until the model finds weights that minimize the loss.\n",
        "\n",
        "---\n",
        "\n",
        "### Example: Linear Regression\n",
        "\n",
        "Loss function (mean squared error):\n",
        "\n",
        "\\[\n",
        "\\text{Loss} = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
        "\\]\n",
        "\n",
        "We take the derivative of the loss with respect to each weight to know how to update it.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Integration in Machine Learning\n",
        "\n",
        "### What Is Integration?\n",
        "\n",
        "**Integration** is about adding up small pieces — like finding the area under a curve.  \n",
        "In ML, we mostly use it in **probability and statistics**.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Uses\n",
        "\n",
        "- Making sure a **probability distribution** adds up to 1:\n",
        "  \n",
        "  \\[\n",
        "  \\int_{-\\infty}^{\\infty} p(x) \\, dx = 1\n",
        "  \\]\n",
        "\n",
        "- Calculating an **expected value** (average):\n",
        "\n",
        "  \\[\n",
        "  \\mathbb{E}[X] = \\int x \\cdot p(x) \\, dx\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### Where Integration Is Used\n",
        "\n",
        "- Probabilistic models (e.g. Bayesian models)\n",
        "- Expected loss or expected predictions\n",
        "- Normalizing distributions (ensuring valid probabilities)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Summary\n",
        "\n",
        "| Concept           | What It Means                                | Why It Matters in ML                     |\n",
        "|------------------|-----------------------------------------------|------------------------------------------|\n",
        "| Derivative        | How fast a function changes                   | Used to update model parameters          |\n",
        "| Gradient          | Derivatives for all parameters                | Tells model how to improve               |\n",
        "| Loss Function     | How wrong the model is                        | We want to minimize this                 |\n",
        "| Integration       | Area under a curve / sum of values            | Used in probabilities and expectations   |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Final Note\n",
        "\n",
        "Derivatives and integration are not just theory — they are used in every machine learning model.  \n",
        "Understanding them helps you know **why your model learns** and how to make it better.\n",
        "\n"
      ],
      "metadata": {
        "id": "LedztZzCI5iK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFjr0tbLRP4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}